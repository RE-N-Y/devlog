---
import Page from "@/src/layouts/Page.astro"
---

<Page>
	<div class="space-y-8">
		<div>
			<h2>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17534">
					Humor Knowledge Enriched Transformer for Understanding Multimodal Humor
				</a>
			</h2>
			<div class="flex">
				<span class="grow">Md Kamrul Hasan, <span class="font-bold">Sangwu Lee</span>, Wasifur Rahman, Amir Zadeh, et al.</span>
				<div class="flex gap-x-2">
					AAAI 2021 <a href="https://github.com/matalvepu/HKT">Code</a>
				</div>
			</div>
			<div class="py-4">
				<h4 class="text-md font-bold">Abstract</h4>
				<p>
					Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as
					incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched
					Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context
					and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and
					sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately
					using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model
					achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets - achieving a new
					state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that
					our model can capture interpretable, humor-inducing patterns from all modalities.
				</p>
			</div>
		</div>
		<div>
			<h2>
				<a href="https://www.jmir.org/2021/10/e26305">Detecting Parkinson Disease Using a Web-Based Speech Task</a>
			</h2>
			<div class="flex">
				<div class="grow">
					<span>Wasifur Rahman*, <span class="font-bold">Sangwu Lee*</span>, Md Saiful Islam, Victor Anthony, et al.</span>
					<br />
					<span class="text-sm">Equal contribution*</span>
				</div>
				<div class="flex gap-x-2">JMLR 2021</div>
			</div>

			<div class="py-4">
				<h4 class="text-md font-bold">Abstract</h4>
				<p>
					Access to neurological care for Parkinson disease (PD) is a rare privilege for millions of people worldwide,
					especially in resource-limited countries. In 2013, there were just 1200 neurologists in India for a population of 1.3
					billion people; in Africa, the average population per neurologist exceeds 3.3 million people. In contrast, 60,000
					people receive a diagnosis of PD every year in the United States alone, and similar patterns of rising PD cases can be
					seen worldwide. In this paper, we propose a web-based framework that can help anyone anywhere around the world record
					a short speech task and analyze the recorded data to screen for PD.
				</p>
			</div>
		</div>
		<div>
			<h2>
				<a href="https://arxiv.org/abs/1908.05787">Integrating Multimodal Information in Large Pretrained Transformers</a>
			</h2>
			<div class="flex">
				<span class="grow">
					Wasifur Rahman, Md. Kamrul Hasan, <span class="font-bold">Sangwu Lee</span>, Amir Zadeh, et al.
				</span>
				<div class="flex gap-x-2">
					ACL 2020 <a href="https://github.com/WasifurRahman/BERT_multimodal_transformer">Code</a>
				</div>
			</div>

			<div class="py-4">
				<h4 class="text-md font-bold">Abstract</h4>
				<p>
					Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art
					performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets
					has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is
					straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal
					language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the
					necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment
					to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal
					data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that
					is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and
					CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the
					sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the
					CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the
					NLP community.
				</p>
			</div>
		</div>
	</div>
</Page>
